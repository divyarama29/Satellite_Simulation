{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Satellite Simulation** \\\n",
    "(Divya Ramachandran,Hadis  Banafsheh, Suvan Kumar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNODAS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/extracting WBD 10\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010105HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010205HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010305HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010405HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010505HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010605HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010705HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010805HP001.nc\n",
      "Input file size is 6935, 3351\n",
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Processing us_ssmv11034tS__T0001TTNATS2024010905HP001.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "start = datetime(2024, 1, 1, tzinfo=timezone.utc)\n",
    "end = datetime(2024, 5, 30, tzinfo=timezone.utc)\n",
    "frame_duration = timedelta(days=1)\n",
    "num_frames = int(1 + (end - start) / frame_duration)\n",
    "\n",
    "# automatically download/extract watershed boundary shapefiles\n",
    "for wbd in [10]:\n",
    "    if not os.path.isfile(f\"WBD_{wbd}_HU2_Shape/Shape/WBDHU2.shp\"):\n",
    "        print(f\"Downloading/extracting WBD {wbd}\")\n",
    "        r = requests.get(\n",
    "            f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/WBD_{wbd}_HU2_Shape.zip\"\n",
    "        )\n",
    "        with open(f\"WBD_{wbd}_HU2_Shape.zip\", \"wb\") as zip_file:\n",
    "            zip_file.write(r.content)\n",
    "        with zipfile.ZipFile(f\"WBD_{wbd}_HU2_Shape.zip\", 'r') as zip_file:\n",
    "            zip_file.extractall(f\"WBD_{wbd}_HU2_Shape\")\n",
    "\n",
    "mo_basin = gpd.read_file(\"WBD_10_HU2_Shape/Shape/WBDHU2.shp\")\n",
    "mo_basin = gpd.GeoSeries(Polygon(mo_basin.iloc[0].geometry.exterior), crs=\"EPSG:4326\")\n",
    "\n",
    "# access us states shapefile\n",
    "us_map = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_20m.zip\")\n",
    "conus = us_map[~us_map.STUSPS.isin([\"AK\", \"HI\", \"PR\"])].to_crs(\"EPSG:4326\")\n",
    "# note: requires gdal (has platform-specific binaries)\n",
    "# recommend conda distribution: `conda install -c conda-forge gdal`\n",
    "from osgeo import gdal\n",
    "\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "dates = pd.date_range(start, end)\n",
    "snodas_dir = \"SNODAS\"\n",
    "if not os.path.exists(snodas_dir):\n",
    "    os.makedirs(snodas_dir)\n",
    "\n",
    "for date in dates:\n",
    "    # Make sure the date is timezone-aware\n",
    "    date = datetime(date.year, date.month, date.day, tzinfo=timezone.utc)\n",
    "\n",
    "    file_label = f\"us_ssmv11034tS__T0001TTNATS{date.strftime('%Y')}{date.strftime('%m')}{date.strftime('%d')}05HP001.nc\"\n",
    "    \n",
    "    # check if file already exists\n",
    "    if os.path.isfile(os.path.join(snodas_dir, file_label)):\n",
    "        print(\"Skipping \" + file_label)\n",
    "        continue\n",
    "    print(\"Processing \" + file_label)\n",
    "    dir_label = f\"SNODAS_{date.strftime('%Y%m%d')}\"\n",
    "    # request the .tar file\n",
    "    r = requests.get(\n",
    "        \"https://noaadata.apps.nsidc.org/NOAA/G02158/masked/\" + \n",
    "        f\"{date.strftime('%Y')}/{date.strftime('%m')}_{date.strftime('%b')}/\" +\n",
    "        dir_label + \".tar\"\n",
    "    )\n",
    "    # do work in temp directory\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # save the .tar file\n",
    "        with open(os.path.join(tmp_dir, dir_label + \".tar\"), \"wb\") as tar_file:\n",
    "            tar_file.write(r.content)\n",
    "        # open and extract the .tar file\n",
    "        with tarfile.open(os.path.join(tmp_dir, dir_label + \".tar\"), \"r\") as tar_file:\n",
    "            tar_file.extractall(tmp_dir)\n",
    "        # iterate through all files\n",
    "        for filename in os.listdir(tmp_dir):\n",
    "            if os.path.isfile(os.path.join(tmp_dir, filename)) and all(s in filename for s in [\"v11034\", \".dat.gz\"]):\n",
    "                # unzip the swe .gz file\n",
    "                with gzip.open(os.path.join(tmp_dir, filename), \"rb\") as gz_in:\n",
    "                    with open(os.path.join(tmp_dir, filename.replace(\".gz\", \"\")), \"wb\") as gz_out:\n",
    "                        shutil.copyfileobj(gz_in, gz_out)\n",
    "                # write the .hdr file\n",
    "                with open(os.path.join(tmp_dir, filename.replace(\".dat.gz\", \".hdr\")), \"w\") as hdr_file:\n",
    "                    hdr_file.write(\n",
    "                        \"ENVI\\n\"\n",
    "                        \"samples = 6935\\n\" +\n",
    "                        \"lines = 3351\\n\" +\n",
    "                        \"bands = 1\\n\" +\n",
    "                        \"header offset = 0\\n\" + \n",
    "                        \"file type = ENVI Standard\\n\" + \n",
    "                        \"data type = 2\\n\" +\n",
    "                        \"interleave = bsq\\n\" +\n",
    "                        \"byte order = 1\"\n",
    "                    )\n",
    "                # run the gdal translator\n",
    "                if date < datetime(2013, 10, 1, tzinfo=timezone.utc):\n",
    "                    command = \" \".join([\n",
    "                        \"gdal_translate\",\n",
    "                        \"-of NetCDF\",\n",
    "                        \"-a_srs EPSG:4326\",\n",
    "                        \"-a_nodata -9999\",\n",
    "                        \"-a_ullr -124.73375000000000 52.87458333333333 -66.94208333333333 24.94958333333333\",\n",
    "                        os.path.join(tmp_dir, filename.replace(\".dat.gz\", \".dat\")),\n",
    "                        os.path.join(snodas_dir, filename.replace(\".dat.gz\", \".nc\"))\n",
    "                    ])\n",
    "                else:\n",
    "                    command = \" \".join([\n",
    "                        \"gdal_translate\",\n",
    "                        \"-of NetCDF\",\n",
    "                        \"-a_srs EPSG:4326\",\n",
    "                        \"-a_nodata -9999\",\n",
    "                        \"-a_ullr -124.73333333333333 52.87500000000000 -66.94166666666667 24.95000000000000\",\n",
    "                        os.path.join(tmp_dir, filename.replace(\".dat.gz\", \".dat\")),\n",
    "                        os.path.join(snodas_dir, filename.replace(\".dat.gz\", \".nc\"))\n",
    "                    ])\n",
    "                if os.system(command) > 0: \n",
    "                    print(f\"Error processing command `{command}`\")\n",
    "\n",
    "print(\"Merging datasets...\")\n",
    "ds = xr.combine_by_coords(\n",
    "    [\n",
    "        rxr.open_rasterio(\n",
    "            os.path.join(\n",
    "                snodas_dir, \n",
    "                f\"us_ssmv11034tS__T0001TTNATS{date.strftime('%Y')}{date.strftime('%m')}{date.strftime('%d')}05HP001.nc\"\n",
    "            )\n",
    "        ).rio.clip(mo_basin.envelope).assign_coords(time=date).expand_dims(dim=\"time\")\n",
    "        for date in dates\n",
    "    ], \n",
    "    combine_attrs=\"drop_conflicts\"\n",
    ").sel(band=1).rename_vars({\"Band1\": \"SWE\"})\n",
    "ds.SWE.attrs[\"long_name\"] = \"Snow Water Equivalent\"\n",
    "ds.SWE.attrs[\"units\"] = \"mm\"\n",
    "\n",
    "print(\"Writing snodas-merged.nc\")\n",
    "ds.to_netcdf(os.path.join(snodas_dir, \"snodas-merged.nc\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Down sampling the resoultio from 1km to 100km\n",
    "from rasterio.enums import Resampling\n",
    "swe = ds.SWE.rio.reproject(ds.SWE.rio.crs, shape=(ds.SWE.shape[1]//100, ds.SWE.shape[2]//100), resampling=Resampling.average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure dimensions of size 1 are squeezed out\n",
    "if 'band' in swe.dims and swe.sizes['band'] == 1:\n",
    "    swe = swe.squeeze('band')\n",
    "\n",
    "# Mask NaN values in SWE\n",
    "swe_masked = swe.where(~np.isnan(swe))\n",
    "\n",
    "# Calculate the absolute SWE difference between consecutive time steps\n",
    "swe_diff_abs = abs(swe_masked.diff(dim='time')).where(~np.isnan(swe_masked.diff(dim='time')))\n",
    "\n",
    "# Add a zero difference for the first time step to match the length\n",
    "swe_diff_abs = xr.concat([xr.zeros_like(swe.isel(time=0)), swe_diff_abs], dim='time')\n",
    "\n",
    "# Set the threshold for SWE change\n",
    "threshold = 15\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the dataset and check for each grid cell\n",
    "for t in range(len(swe_diff_abs.time)):\n",
    "    # Extract the SWE difference for the current time step\n",
    "    swe_diff_time = swe_diff_abs.isel(time=t)\n",
    "    \n",
    "    # Check if any grid cell exceeds the threshold\n",
    "    grid_cells_exceeding_threshold = swe_diff_time.where(swe_diff_time > threshold, drop=True)\n",
    "    \n",
    "    if not grid_cells_exceeding_threshold.isnull().all():\n",
    "        # Get the date of the current time step\n",
    "        date = swe['time'].isel(time=t).values.astype('datetime64[D]')\n",
    "        \n",
    "        # Convert to a Python datetime object to access 'year'\n",
    "        date_obj = pd.to_datetime(date)\n",
    "        \n",
    "        # Calculate the number of days since the start of the year (January 1st)\n",
    "        day_from_start = (date_obj - datetime(date_obj.year, 1, 1)).days + 1\n",
    "        \n",
    "        # Append the results\n",
    "        results.append([date, day_from_start])\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "dates_df = pd.DataFrame(results, columns=[\"Date\", \"Days_from_start\"])\n",
    "\n",
    "# Calculate the interarrival days by taking the difference between consecutive dates\n",
    "dates_df['Interarrival_days'] = dates_df['Date'].diff().fillna(pd.Timedelta(0)).dt.days\n",
    "\n",
    "# Remove the first row\n",
    "dates_df = dates_df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# output_csv = 'dates_with_interarrival.csv'\n",
    "# dates_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Print confirmation and show a preview of the DataFrame\n",
    "# print(f\"Dates where SWE change exceeds the threshold saved to '{output_csv}'.\")\n",
    "print(dates_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def bootstrap_interarrivals(dates_df, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform bootstrap sampling on the inter-arrival times and calculate the mean and confidence intervals.\n",
    "\n",
    "    Parameters:\n",
    "    dates_df (DataFrame): DataFrame containing the inter-arrival days data.\n",
    "    n_iterations (int): The number of bootstrap samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the lower and upper bounds of the 95% confidence interval for the mean.\n",
    "    \"\"\"\n",
    "    # Create a list to store the bootstrap samples\n",
    "    bootstrap_samples = []\n",
    "    bootstrap_means = []\n",
    "\n",
    "    # Perform bootstrap sampling\n",
    "    for i in range(n_iterations):\n",
    "        # Sample with replacement from the 'Interarrival_days' column\n",
    "        sample = dates_df['Interarrival_days'].sample(frac=1, replace=True)\n",
    "        bootstrap_samples.append(sample)  \n",
    "        sample_mean = np.mean(sample) \n",
    "        bootstrap_means.append(sample_mean)\n",
    "\n",
    "    # Calculate the 95% confidence interval (using the 2.5th and 97.5th percentiles)\n",
    "    lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "def image_request(dates_df):\n",
    "    \"\"\"\n",
    "    Function to get a random inter-arrival time from bootstrap samples.\n",
    "    This function randomly selects a sample from the 'Interarrival_days' column in the bootstrap dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    dates_df (DataFrame): DataFrame containing the inter-arrival days data.\n",
    "    \n",
    "    Returns:\n",
    "    timedelta: The inter-arrival time (in days) for the next satellite request.\n",
    "    \"\"\"\n",
    "    # Sample a random inter-arrival time from the bootstrap samples\n",
    "    inter_arrival_time = np.random.choice(dates_df['Interarrival_days'])\n",
    "    \n",
    "    # Convert to plain Python int to avoid the TypeError with numpy.int64\n",
    "    inter_arrival_time = int(inter_arrival_time)\n",
    "    \n",
    "    return timedelta(days=inter_arrival_time)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your `dates_df` already loaded\n",
    "# Perform bootstrap analysis\n",
    "lower_bound, upper_bound = bootstrap_interarrivals(dates_df, n_iterations=1000)\n",
    "print(f\"95% Confidence Interval for the Mean: ({lower_bound}, {upper_bound})\")\n",
    "\n",
    "# Now, call image_request function with dates_df\n",
    "image_delta = image_request(dates_df)\n",
    "print(f\"Next Image Request will be in: {image_delta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes made on 7th December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tatc\n",
    "import rioxarray as rxr\n",
    "#import tatc[examples]\n",
    "from tatc.schemas import TwoLineElements\n",
    "from tatc.schemas import Point\n",
    "from tatc.analysis import collect_observations\n",
    "from tatc import utils\n",
    "from tatc.schemas import Instrument\n",
    "from tatc.schemas import WalkerConstellation, SunSynchronousOrbit\n",
    "from tatc.utils import (\n",
    "    swath_width_to_field_of_regard,\n",
    "    along_track_distance_to_access_time,\n",
    ")\n",
    "import datetime\n",
    "from tatc.analysis import collect_multi_observations\n",
    "from tatc.schemas import Satellite\n",
    "from datetime import datetime, timedelta, timezone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Satellites and TLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa20_tle = [\n",
    "    \"1 43013U 17073A   22195.78278435  .00000038  00000+0  38919-4 0  9996\",\n",
    "    \"2 43013  98.7169 133.9110 0001202  63.8768 296.2532 14.19561306241107\",\n",
    "]\n",
    "CAPELLA_13_tle= [ \n",
    "    \"1 60419U 24142A   24326.56272859  .00014736  00000+0  16272-2 0  9994\",\n",
    "    \"2 60419  53.0086  37.0038 0001423  85.1678 274.9464 14.87633793 15171\",\n",
    "]\n",
    "\n",
    "CAPELLA_14_tle= [ \n",
    "    \"1 59444U 24066C   24326.41256276  .00017846  00000+0  17602-2 0  9997\",\n",
    "    \"2 59444  45.6101 354.1920 0001417  63.9982 296.1042 14.91842553 26306\",\n",
    "]\n",
    "\n",
    "CAPELLA_15_tle= [ \n",
    "    \"1 60544U 24149CE  24326.63431561  .00009918  00000+0  86496-3 0  9997\",\n",
    "    \"2 60544  97.7350  40.7817 0005870 174.5984 185.5302 14.97041464 14455\",\n",
    "]\n",
    "\n",
    "CAPELLA_11_tle= [ \n",
    "    \"1 57693U 23126A   24326.55257719  .00001335  00000+0  20605-3 0  9999\",\n",
    "    \"2 57693  53.0084 132.6964 0004021 153.2687 206.8497 14.79537557 67352\",\n",
    "]\n",
    "\n",
    "CAPELLA_09_tle= [ \n",
    "    \"1 55910U 23035C   24326.19469929  .00080387  00000+0  28081-2 0  9995\",\n",
    "    \"2 55910  43.9854 265.6943 0016492 143.7069 216.4921 15.28530799 92535\",\n",
    "]\n",
    "\n",
    "CAPELLA_10_tle= [ \n",
    "    \"1 55909U 23035B   24326.33838605  .00062564  00000+0  26624-2 0  9998\",\n",
    "    \"2 55909  43.9916 278.3602 0014472 119.8274 240.4032 15.22108149 92391\",\n",
    "]\n",
    "\n",
    "# Design Alternatives\n",
    "\n",
    "GRACE_FO_1_tle = [             \n",
    "    \"1 43476U 18047A   24338.59478596  .00008469  00000+0  27203-3 0  9990\",\n",
    "    \"2 43476  88.9749 275.5963 0014875 124.0249 236.2418 15.32050354363786\",\n",
    "]\n",
    "\n",
    "GRACE_FO_2_tle = [             \n",
    "    \"1 43477U 18047B   24338.52972907  .00008473  00000+0  27213-3 0  9995\",\n",
    "    \"2 43477  88.9748 275.6044 0014834 124.1677 236.0984 15.32053872363771\",\n",
    "]\n",
    "\n",
    "capella_09_orbit = TwoLineElements(tle=CAPELLA_09_tle)\n",
    "capella_10_orbit = TwoLineElements(tle=CAPELLA_10_tle)\n",
    "capella_11_orbit = TwoLineElements(tle=CAPELLA_11_tle)\n",
    "capella_13_orbit = TwoLineElements(tle=CAPELLA_13_tle)\n",
    "capella_14_orbit = TwoLineElements(tle=CAPELLA_14_tle)\n",
    "capella_15_orbit = TwoLineElements(tle=CAPELLA_15_tle)\n",
    "grace_fo_1_orbit = TwoLineElements(tle=GRACE_FO_1_tle)\n",
    "grace_fo_2_orbit = TwoLineElements(tle=GRACE_FO_2_tle)\n",
    "\n",
    "#noaa20_orbit = TwoLineElements(tle=noaa20_tle)\n",
    "\n",
    "# Defining Instrument\n",
    "capella_sar_for = utils.swath_width_to_field_of_regard(500e3, 30e3)\n",
    "capella_sar = Instrument(name=\"SAR\", field_of_regard=capella_sar_for)\n",
    "\n",
    "# Instrument for GRACE\n",
    "grace_LRR_for = utils.swath_width_to_field_of_regard(490e3, 300e3)\n",
    "grace_LRR = Instrument(name=\"LRR\", field_of_regard=grace_LRR_for)\n",
    "\n",
    "# Defining Satellite\n",
    "\n",
    "Capella_09 = Satellite(name=\"Capella 9\", orbit=capella_09_orbit, instruments=[capella_sar])\n",
    "Capella_10 = Satellite(name=\"Capella 10\", orbit=capella_10_orbit, instruments=[capella_sar])\n",
    "Capella_11 = Satellite(name=\"Capella 11\", orbit=capella_11_orbit, instruments=[capella_sar])\n",
    "Capella_13 = Satellite(name=\"Capella 13 \", orbit=capella_13_orbit, instruments=[capella_sar])\n",
    "Capella_14 = Satellite(name=\"Capella 14\", orbit=capella_14_orbit, instruments=[capella_sar])\n",
    "Capella_15 = Satellite(name=\"Capella 15\", orbit=capella_15_orbit, instruments=[capella_sar])\n",
    "\n",
    "\n",
    "# Design Alternatives\n",
    "Grace_fo_1 = Satellite(name=\"GRACE FO-1 \", orbit=grace_fo_1_orbit, instruments=[grace_LRR])\n",
    "Grace_fo_2 = Satellite(name=\"GRACE FO-2 \", orbit=grace_fo_2_orbit, instruments=[grace_LRR])\n",
    "\n",
    "# Base Constellation and Design alternatives\n",
    "Capella_const = [Capella_09,Capella_10,Capella_11,Capella_13,Capella_14,Capella_15]\n",
    "Design_1 = [Capella_09,Capella_10,Capella_11,Capella_13,Capella_14,Capella_15,Grace_fo_1]\n",
    "Design_2 = [Capella_09,Capella_10,Capella_11,Capella_13,Capella_14,Capella_15,Grace_fo_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining centroid of missouri basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    POINT (-102.69201 43.50661)\n",
       "dtype: geometry"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "path_shp = 'C:\\\\Users\\\\dramach6\\\\ASU Dropbox\\\\Divya Ramachandran\\\\SOS Planning\\\\Downloaded_files\\\\Mo_basin_shp\\\\'\n",
    "# Automatically download/extract watershed boundary shapefiles\n",
    "for wbd in [10]:\n",
    "    if not os.path.isfile(path_shp+f\"WBD_{wbd}_HU2_Shape/Shape/WBDHU2.shp\"):\n",
    "        print(f\"Downloading/extracting WBD {wbd}\")\n",
    "        r = requests.get(\n",
    "            f\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/WBD_{wbd}_HU2_Shape.zip\"\n",
    "        )\n",
    "        with open(path_shp + f\"WBD_{wbd}_HU2_Shape.zip\", \"wb\") as zip_file:\n",
    "            zip_file.write(r.content)\n",
    "        with zipfile.ZipFile(path_shp+f\"WBD_{wbd}_HU2_Shape.zip\", 'r') as zip_file:\n",
    "            zip_file.extractall(path_shp+f\"WBD_{wbd}_HU2_Shape\")\n",
    "\n",
    "mo_basin = gpd.read_file(path_shp+\"WBD_10_HU2_Shape/Shape/WBDHU2.shp\")\n",
    "mo_basin = gpd.GeoSeries(Polygon(mo_basin.iloc[0].geometry.exterior), crs=\"EPSG:4326\")\n",
    "mo_basin = mo_basin.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Finding centroid of Missouri Basin\n",
    "\n",
    "centroid = mo_basin.to_crs('+proj=cea').centroid.to_crs(mo_basin.crs)\n",
    "#centroid = mo_basin.geometry.centroid\n",
    "centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduling Arrivals, that is, imaging requests\\\n",
    "Using TAT-C to generate observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import warnings\n",
    "\n",
    "# def imaging_request():\n",
    "#     return timedelta(np.random.exponential(1/0.16))\n",
    "\n",
    "def constellation_capacity():\n",
    "    return np.random.rand()\n",
    "\n",
    "# Generating observations\n",
    "\n",
    "from tatc.schemas import Point\n",
    "missouri = Point(id=0,latitude=43.50661,longitude=-102.69201)\n",
    "\n",
    "# Collecting multi observations ( 5 days from start)\n",
    "\n",
    "def observations(start,counter,constellation):\n",
    "\n",
    "    #start = datetime(year=2024, month=1, day=1, hour=12, tzinfo=timezone.utc)   \n",
    "    end = start + timedelta(days=10)\n",
    "    results = collect_multi_observations(missouri, constellation, start, end) \n",
    "    #display(results)  \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)  # Ignore UserWarnings\n",
    "        next_obs = (results['epoch'][counter]).to_pydatetime()  \n",
    "    #next_obs = (results['epoch'][counter]).to_pydatetime()\n",
    "    sat = results['satellite'][counter]\n",
    "    return (next_obs,sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HADIS CODE\\\n",
    "Note \n",
    "1. Refer to imaging_requests module above\n",
    "2. Ensure all arrivals are generated at time 0:0000\n",
    "3. Change N value for testing, currently it is set to 50, that will take time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input modelling here\n",
    "# Hadis's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2024, 12, 8, 2, 21, 43, 128747, tzinfo=datetime.timezone.utc),\n",
       " 'Capella 13 ')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations(datetime.now(timezone.utc),0,Capella_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrete Event Simulation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Model\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def simu(constellation):\n",
    "    t = datetime.now(timezone.utc)\n",
    "    end = t + timedelta(61)\n",
    "    image_delta = image_request()\n",
    "    User_request = t + image_delta\n",
    "    inf_time = datetime.max.replace(tzinfo=timezone.utc)\n",
    "    Next_Observation = inf_time\n",
    "    Next_Opportunity = inf_time\n",
    "    N = 0\n",
    "    const_capacity = 0\n",
    "    threshold = 0.75\n",
    "    Type = \"\"\n",
    "    counter = 0\n",
    "    window_close = 2\n",
    "    arr_user_request = []\n",
    "    arr_obs_completion = []\n",
    "    i = 0\n",
    "    expiration_date = []\n",
    "    request_status = ''\n",
    "    arr_request_status = []\n",
    "\n",
    "    k = 20\n",
    "\n",
    "    print(\"{:^10}{:^15}{:^35}{:^35}{:^35}{:^25}{:^35}{:^35}{:^10}{:^20}\".format(\"iteration\",\"Type\" ,\"Time\", \"delta\",\"Next user Request\" ,\"Number of active request\",\"Next Opportunity\", \"Next Observation\",\"Status\",\"Constellation Capacity\"))\n",
    "    print(\"\\n\")\n",
    "    #for i in range(k):\n",
    "    while t <= end:\n",
    "        formatted_image_delta = f\"{image_delta.days} days, {image_delta.seconds // 3600:02}:{(image_delta.seconds % 3600) // 60:02}:{image_delta.seconds % 60:02}\"\n",
    "        text = \"{:^10}{:^15}{:^35}{:^35}{:^35}{:^25}{:^35}{:^35}{:^10}{:^20.2f}\"\n",
    "        print(text.format(i,Type,t.strftime(\"%Y-%m-%d %H:%M:%S\"),formatted_image_delta,User_request.strftime(\"%Y-%m-%d %H:%M:%S\"),N,Next_Opportunity.strftime(\"%Y-%m-%d %H:%M:%S\"),Next_Observation.strftime(\"%Y-%m-%d %H:%M:%S\"),request_status,const_capacity))\n",
    "        \n",
    "        t = min(User_request,Next_Observation)\n",
    "        if t < Next_Observation:\n",
    "            Type = 'User_Request'\n",
    "            expiration = t+timedelta(window_close)\n",
    "            arr_user_request.append(t)\n",
    "            expiration_date.append(expiration)\n",
    "            image_delta = image_request()\n",
    "            User_request = t + image_delta        \n",
    "            const_capacity = 0\n",
    "            request_status = 'Active'\n",
    "            N+=1\n",
    "            if N>0 and Next_Observation==inf_time:\n",
    "                counter = 0\n",
    "                obs = observations(t,counter,constellation)\n",
    "                #print(obs)\n",
    "                #Next_Observation = t + obs[0]\n",
    "                Next_Observation = obs[0]\n",
    "                Next_Observation_satellite = obs[1]\n",
    "                #Next_Opportunity = t + obs[0]\n",
    "                Next_Opportunity = obs[0]\n",
    "                if Next_Observation > expiration:\n",
    "                    N = 0\n",
    "                    Next_Observation = inf_time\n",
    "                    request_status = 'Rejected'\n",
    "            arr_request_status.append(request_status)          \n",
    "\n",
    "\n",
    "        elif t <= expiration:\n",
    "            Type = 'Obs Opportunity'\n",
    "            const_capacity = constellation_capacity()\n",
    "            if const_capacity < 0.75:\n",
    "                N-=1\n",
    "                request_status = 'Completed'\n",
    "                arr_obs_completion.append([t,Next_Observation_satellite,request_status])\n",
    "                Next_Observation = inf_time\n",
    "                Next_Opportunity = inf_time\n",
    "                N = 0\n",
    "                counter =0\n",
    "            else:\n",
    "                counter = 0\n",
    "                obs = observations(t,counter,constellation)                \n",
    "                #Next_Observation = t + obs[0]\n",
    "                Next_Observation = obs[0]\n",
    "                Next_Observation_satellite = obs[1]\n",
    "                #Next_Opportunity = t + obs[0]\n",
    "                Next_Opportunity = obs[0]\n",
    "                if Next_Observation > expiration:\n",
    "                    N = 0\n",
    "                    Next_Observation = inf_time\n",
    "                    request_status = 'Rejected'\n",
    "\n",
    "        else:\n",
    "            request_status = 'Rejected'\n",
    "            Next_Observation = inf_time\n",
    "            N = 0\n",
    "\n",
    "\n",
    "        i = i + 1\n",
    "      \n",
    "    return arr_user_request, arr_obs_completion,expiration_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataframe for Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store user requests, completion times, and satellite assignments\n",
    "def create_observation_dataframe(user_requests, obs_completion_data,request_expiration):\n",
    "    # Initialize lists for the DataFrame\n",
    "    completion_dates = []\n",
    "    satellites = []\n",
    "    status = []    \n",
    "    \n",
    "    for req in user_requests:\n",
    "        # Find the first observation completion that is on or after the user request\n",
    "        matched_completion = None\n",
    "        matched_satellite = None\n",
    "        req_status = None\n",
    "        for comp, sat, stat in obs_completion_data:\n",
    "            if req <= comp:\n",
    "                matched_completion = comp\n",
    "                matched_satellite = sat\n",
    "                req_status = stat\n",
    "                break\n",
    "        \n",
    "        # Append the matched values\n",
    "        completion_dates.append(matched_completion)\n",
    "        satellites.append(matched_satellite)\n",
    "        status.append(req_status)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"User Request\": user_requests,\n",
    "        \"Request Expiration\":request_expiration,\n",
    "        \"Request Status\": status,\n",
    "        \"Observation Completion\": completion_dates,\n",
    "        \"Satellite\": satellites\n",
    "    })\n",
    "\n",
    "    # Convert the columns to datetime if they are not already\n",
    "    df[\"User Request\"] = pd.to_datetime(df[\"User Request\"])\n",
    "    df[\"Observation Completion\"] = pd.to_datetime(df[\"Observation Completion\"])\n",
    "    df = df.iloc[:-1]\n",
    "    \n",
    "    # Calculate the difference between User Request and Observation Completion  \n",
    "\n",
    "    df[\"Time Difference\"] = df.apply(\n",
    "    lambda row: ((row[\"Observation Completion\"] - row[\"User Request\"]).seconds)/3600 \n",
    "    if pd.notna(row[\"Observation Completion\"]) else np.nan,\n",
    "    axis=1\n",
    "    )\n",
    "    df[\"Within 2 Days\"] = df[\"Time Difference\"].apply(\n",
    "    lambda x: 1 if pd.notna(x) and x <= 48 else 0\n",
    "    )    \n",
    "    \n",
    "    if len(df) > 0:\n",
    "        service = df[\"Within 2 Days\"].sum() / len(df)\n",
    "    else:\n",
    "        service = np.nan\n",
    "\n",
    "    # Ensure non-null values exist for Average_TAT calculation\n",
    "    non_null_time_diff = df[\"Time Difference\"].dropna()\n",
    "    if len(non_null_time_diff) > 0:\n",
    "        tat = non_null_time_diff.sum() / len(non_null_time_diff)\n",
    "    else:\n",
    "        tat = np.nan\n",
    "\n",
    "    # Replace NaN values for missing completion dates with a placeholder (e.g., \"NA\")\n",
    "    df[\"Observation Completion\"] = df[\"Observation Completion\"].fillna(\"NA\") \n",
    "    display(df)   \n",
    "\n",
    "    return service,tat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model for N iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "t = []\n",
    "mean_tat = []\n",
    "std_tat = []\n",
    "mean_service = []\n",
    "std_service = []\n",
    "Design = [Capella_const,Design_1,Design_2]\n",
    "Design_name = ['Capella_const','Grace_fo_1','Grace_fo_2']\n",
    "N = 50\n",
    "\n",
    "for j in Design:\n",
    "   print(j)\n",
    "   for i in range(N):\n",
    "      sim = simu(j)\n",
    "      arr_user_request = sim[0]   \n",
    "      arr_obs_completion = sim[1]\n",
    "      # print(len(arr_user_request))\n",
    "      # print(len(arr_obs_completion))\n",
    "      expiration_date = sim[2]  \n",
    "      #arr_user_request = arr_user_request[:-1] \n",
    "      df = create_observation_dataframe(arr_user_request, arr_obs_completion,expiration_date)\n",
    "      s.append(df[0])\n",
    "      t.append(df[1])\n",
    "\n",
    "   service = np.array(s).flatten()\n",
    "   tat = np.array(t).flatten()\n",
    "   mean_tat.append(np.nanmean(tat))\n",
    "   std_tat.append(np.nanstd(tat))\n",
    "   mean_service.append(np.nanmean(service))\n",
    "   std_service.append(np.nanstd(service))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Capella_const\n",
      "Mean TAT(hours): 11.702513691794318\n",
      "Standard Deviation TAT(hours): 2.4541509969616397\n",
      "Mean Service: 0.9299531775414128\n",
      "Standard Deviation Service: 0.10633928815717024\n",
      "\n",
      " \n",
      "Grace_fo_1\n",
      "Mean TAT(hours): 11.252170959727\n",
      "Standard Deviation TAT(hours): 2.53638786060494\n",
      "Mean Service: 0.9356670649611826\n",
      "Standard Deviation Service: 0.10055279454039885\n",
      "\n",
      " \n",
      "Grace_fo_2\n",
      "Mean TAT(hours): 10.752784908990188\n",
      "Standard Deviation TAT(hours): 2.4943089685080784\n",
      "Mean Service: 0.9415207080501201\n",
      "Standard Deviation Service: 0.0942568122454\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for i in range(len(Design_name)):\n",
    "    print(\"\\n \")\n",
    "    print(Design_name[i])\n",
    "    print(f\"Mean TAT(hours): {mean_tat[i]}\")\n",
    "    print(f\"Standard Deviation TAT(hours): {std_tat[i]}\")\n",
    "    print(f\"Mean Service: {mean_service[i]}\")\n",
    "    print(f\"Standard Deviation Service: {std_service[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute performance \\\n",
    "Prediction and Confidence Interval, 95% confidence levels, N = 50, t = 2.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Capella_const\n",
      "Confidence Interval TAT(hours): (11.005598412092185, 12.39942897149645)\n",
      "Prediction Interval TAT(hours): (6.725543101043617, 16.67948428254502)\n",
      "Confidence Interval Service: (0.8997555756655987, 0.9601507794172269)\n",
      "Prediction Interval Service: (0.7142991650316548, 1.1456071900511708)\n",
      "Number of Samples TAT:92.54963158557844\n",
      "Number of Samples Service:17.37639304836008\n",
      "\n",
      " \n",
      "Grace_fo_1\n",
      "Confidence Interval TAT(hours): (10.531902542056278, 11.972439377397722)\n",
      "Prediction Interval TAT(hours): (6.108425605591731, 16.39591631386227)\n",
      "Confidence Interval Service: (0.9071126773058542, 0.9642214526165109)\n",
      "Prediction Interval Service: (0.7317479491997879, 1.1395861807225771)\n",
      "Number of Samples TAT:98.85609839358256\n",
      "Number of Samples Service:15.536758809734835\n",
      "\n",
      " \n",
      "Grace_fo_2\n",
      "Confidence Interval TAT(hours): (10.044465806129985, 11.461104011850392)\n",
      "Prediction Interval TAT(hours): (5.694374731344357, 15.811195086636019)\n",
      "Confidence Interval Service: (0.9147542161956415, 0.9682871999045988)\n",
      "Prediction Interval Service: (0.7503697221881859, 1.1326716939120542)\n",
      "Number of Samples TAT:95.60324435290867\n",
      "Number of Samples Service:13.652042443423788\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Design_name)):\n",
    "    print(\"\\n \")\n",
    "    print(Design_name[i])\n",
    "    t = 2.008\n",
    "    confidence_tat = (t * std_tat[i])/np.sqrt(50)\n",
    "    prediction_tat = (t * std_tat[i])*np.sqrt(1 + (1/50))\n",
    "    confidence_service = (t * std_service[i])/np.sqrt(50)\n",
    "    prediction_service = (t * std_service[i])*np.sqrt(1 + (1/50))\n",
    "    \n",
    "    Error_threshold_TAT = 0.5 # 30 minute threshold for TAT\n",
    "    Error_threshold_Service = 0.05 # 0.05 threshold for service levels\n",
    "    N_tat = (1.96 * std_tat[i]/Error_threshold_TAT)**2\n",
    "    N_service = (1.96 * std_service[i]/Error_threshold_Service)**2 \n",
    "\n",
    "    print(f\"Confidence Interval TAT(hours): {mean_tat[i] - confidence_tat,mean_tat[i] + confidence_tat }\")\n",
    "    print(f\"Prediction Interval TAT(hours): {mean_tat[i] - prediction_tat,mean_tat[i] + prediction_tat}\")\n",
    "    print(f\"Confidence Interval Service: {mean_service[i] - confidence_service,mean_service[i] + confidence_service }\")\n",
    "    print(f\"Prediction Interval Service: {mean_service[i] - prediction_service,mean_service[i] + prediction_service}\") \n",
    "    print(f\"Number of Samples TAT:{N_tat}\") \n",
    "    print(f\"Number of Samples Service:{N_service}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUVAN CODE \\\n",
    "Note :\\\n",
    "1. Please check the CRN method for checking relative performance(I'm not sure which one to use, currently different random numbers are used for each design alternative) \\\n",
    "2. Also, can you make computing t value using a library rather than keeping it hardcoded?, also change in the above code block for absolute performance \\\n",
    "3. Ouput plots - can you use the same flow as professor's slides for project demonstration?, lmk if you need anything from me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Performance \\\n",
    "Comparing all 3 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
